{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59089c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-23 16:19:37.342445: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-23 16:19:49.188349: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-04-23 16:19:49.188602: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-04-23 16:19:49.188638: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n",
      "0.29.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import tensorflow_io as tfio\n",
    "print(tfio.__version__)\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21721c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>participant_id</th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>sign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/train_landmark_files/26734/1000035562....</td>\n",
       "      <td>26734</td>\n",
       "      <td>1000035562</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/train_landmark_files/28656/1000106739....</td>\n",
       "      <td>28656</td>\n",
       "      <td>1000106739</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/train_landmark_files/16069/100015657.p...</td>\n",
       "      <td>16069</td>\n",
       "      <td>100015657</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/train_landmark_files/25571/1000210073....</td>\n",
       "      <td>25571</td>\n",
       "      <td>1000210073</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/train_landmark_files/62590/1000240708....</td>\n",
       "      <td>62590</td>\n",
       "      <td>1000240708</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  participant_id  \\\n",
       "0  ../data/train_landmark_files/26734/1000035562....           26734   \n",
       "1  ../data/train_landmark_files/28656/1000106739....           28656   \n",
       "2  ../data/train_landmark_files/16069/100015657.p...           16069   \n",
       "3  ../data/train_landmark_files/25571/1000210073....           25571   \n",
       "4  ../data/train_landmark_files/62590/1000240708....           62590   \n",
       "\n",
       "   sequence_id  sign  \n",
       "0   1000035562    25  \n",
       "1   1000106739   232  \n",
       "2    100015657    48  \n",
       "3   1000210073    23  \n",
       "4   1000240708   164  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train.csv file\n",
    "df = pd.read_csv(\"../data/train.csv\")\n",
    "\n",
    "# Get labels 2 id\n",
    "with open(\"../data/sign_to_prediction_index_map.json\") as f:\n",
    "    label2id = json.load(f)\n",
    "    \n",
    "def add_path(row):\n",
    "    return \"../data/\"+row.path\n",
    "    \n",
    "df[\"sign\"] = df[\"sign\"].apply(lambda sign: label2id[sign])\n",
    "df[\"path\"] = df.apply(lambda row: add_path(row), axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e123f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2044\n"
     ]
    }
   ],
   "source": [
    "for participant_id, tmp_df in df.groupby(\"participant_id\"):\n",
    "    print(participant_id)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9febde5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>participant_id</th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>sign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>../data/train_landmark_files/2044/1001950812.p...</td>\n",
       "      <td>2044</td>\n",
       "      <td>1001950812</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>../data/train_landmark_files/2044/1002091184.p...</td>\n",
       "      <td>2044</td>\n",
       "      <td>1002091184</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>../data/train_landmark_files/2044/1002092995.p...</td>\n",
       "      <td>2044</td>\n",
       "      <td>1002092995</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>../data/train_landmark_files/2044/1003433922.p...</td>\n",
       "      <td>2044</td>\n",
       "      <td>1003433922</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>../data/train_landmark_files/2044/1004220951.p...</td>\n",
       "      <td>2044</td>\n",
       "      <td>1004220951</td>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94355</th>\n",
       "      <td>../data/train_landmark_files/2044/994928291.pa...</td>\n",
       "      <td>2044</td>\n",
       "      <td>994928291</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94395</th>\n",
       "      <td>../data/train_landmark_files/2044/996571864.pa...</td>\n",
       "      <td>2044</td>\n",
       "      <td>996571864</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94404</th>\n",
       "      <td>../data/train_landmark_files/2044/996830210.pa...</td>\n",
       "      <td>2044</td>\n",
       "      <td>996830210</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94438</th>\n",
       "      <td>../data/train_landmark_files/2044/998355330.pa...</td>\n",
       "      <td>2044</td>\n",
       "      <td>998355330</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94442</th>\n",
       "      <td>../data/train_landmark_files/2044/998713046.pa...</td>\n",
       "      <td>2044</td>\n",
       "      <td>998713046</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4810 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    path  participant_id  \\\n",
       "46     ../data/train_landmark_files/2044/1001950812.p...            2044   \n",
       "52     ../data/train_landmark_files/2044/1002091184.p...            2044   \n",
       "53     ../data/train_landmark_files/2044/1002092995.p...            2044   \n",
       "87     ../data/train_landmark_files/2044/1003433922.p...            2044   \n",
       "105    ../data/train_landmark_files/2044/1004220951.p...            2044   \n",
       "...                                                  ...             ...   \n",
       "94355  ../data/train_landmark_files/2044/994928291.pa...            2044   \n",
       "94395  ../data/train_landmark_files/2044/996571864.pa...            2044   \n",
       "94404  ../data/train_landmark_files/2044/996830210.pa...            2044   \n",
       "94438  ../data/train_landmark_files/2044/998355330.pa...            2044   \n",
       "94442  ../data/train_landmark_files/2044/998713046.pa...            2044   \n",
       "\n",
       "       sequence_id  sign  \n",
       "46      1001950812   142  \n",
       "52      1002091184    67  \n",
       "53      1002092995    25  \n",
       "87      1003433922   206  \n",
       "105     1004220951   247  \n",
       "...            ...   ...  \n",
       "94355    994928291    81  \n",
       "94395    996571864   149  \n",
       "94404    996830210    41  \n",
       "94438    998355330    31  \n",
       "94442    998713046    79  \n",
       "\n",
       "[4810 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dadf77f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cc8ae2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49445    4968\n",
       "61333    4900\n",
       "36257    4896\n",
       "16069    4848\n",
       "26734    4841\n",
       "55372    4826\n",
       "2044     4810\n",
       "37779    4782\n",
       "32319    4753\n",
       "29302    4722\n",
       "22343    4677\n",
       "53618    4656\n",
       "37055    4648\n",
       "28656    4563\n",
       "62590    4563\n",
       "34503    4545\n",
       "27610    4275\n",
       "25571    3865\n",
       "18796    3502\n",
       "4718     3499\n",
       "30680    3338\n",
       "Name: participant_id, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.participant_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8505858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([26734, 28656, 16069, 25571, 62590, 32319, 37055, 29302, 49445,\n",
       "       36257, 22343, 27610, 61333, 53618, 34503, 18796,  4718, 55372,\n",
       "        2044, 37779, 30680])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.participant_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c11e5640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/train_landmark_files/26734/1000035562.parquet 25\n",
      "12489\n"
     ]
    }
   ],
   "source": [
    "paths = df.path.values\n",
    "labels = df.sign.values\n",
    "\n",
    "NUM_REPS = 543\n",
    "\n",
    "for path, label in zip(paths, labels):\n",
    "    print(path, label)\n",
    "    frames_df = pd.read_parquet(path)[[\"x\", \"y\", \"z\"]]\n",
    "    print(len(frames_df))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeae392",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrecords_dir = \"../data/tfrecords-participants\"\n",
    "\n",
    "num_samples = 4096\n",
    "num_tfrecords = len(df) // num_samples\n",
    "if len(df) % num_samples:\n",
    "    num_tfrecords += 1  # add one record if there are any remaining samples\n",
    "\n",
    "if not os.path.exists(tfrecords_dir):\n",
    "    os.makedirs(tfrecords_dir)  # creating TFRecords output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27e15e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(num_tfrecords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2067d183",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe88610",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=num_tfrecords, random_state=None, shuffle=False)\n",
    "\n",
    "stratified_labels = {}\n",
    "\n",
    "for i, (_, test_index) in enumerate(skf.split(paths, labels)):\n",
    "    print(f\"Fold {i}:\")\n",
    "    print(f\"  Test:  index={test_index}\")\n",
    "    \n",
    "    stratified_labels[i] = test_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1913c749",
   "metadata": {},
   "outputs": [],
   "source": [
    "stratified_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5b5930",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in stratified_labels.items():\n",
    "    split_paths = paths[v]\n",
    "    split_labels = labels[v]\n",
    "    \n",
    "    for path, label in zip(split_paths, split_labels):\n",
    "        frames = pd.read_parquet(path)[[\"x\", \"y\", \"z\"]].values.astype(np.float32)\n",
    "        n_frames = len(frames)/543\n",
    "        \n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec2d895",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7165d1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372241c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78000935",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frames.reshape(int(n_frames), 543, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7d534a",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = tf.io.serialize_tensor(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900efc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.Feature(bytes_list=tf.train.BytesList(value=[st.numpy()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051d1c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = tf.io.parse_tensor(st, out_type=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4e7ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e09aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544e70aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "913f99a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "\n",
    "def int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "def float_sequence(sequence):\n",
    "    \"\"\"Returns a list of float_list from a float / double.\"\"\"\n",
    "    feature_list = [tf.train.Feature(float_list=tf.train.FloatList(value=value.tolist())) for value in sequence]\n",
    "    return tf.train.FeatureList(feature=feature_list)\n",
    "\n",
    "\n",
    "def bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value.numpy()]))\n",
    "\n",
    "\n",
    "def serialize_sequence(sequence):\n",
    "    \"\"\"Serialize the multidimentional tensor\"\"\"\n",
    "    return tf.io.serialize_tensor(sequence)\n",
    "\n",
    "\n",
    "def parse_sequence(serialized_sequence):\n",
    "    return tf.io.parse_tensor(\n",
    "        serialized_sequence,\n",
    "        out_type=tf.float16,\n",
    "    )\n",
    "\n",
    "\n",
    "def create_example(n_frames, sequence, label):\n",
    "    feature = {\n",
    "        \"n_frames\": float_feature(n_frames),\n",
    "        \"frames\": bytes_feature(serialize_sequence(frames)),\n",
    "        \"label\": int64_feature(label),\n",
    "    }\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "\n",
    "# def create_example(n_frame, seq, label):\n",
    "#     sequence_features = tf.train.FeatureLists(feature_list={\"frames\": float_sequence(frames)})\n",
    "\n",
    "#     context_features = tf.train.Features(feature = {\n",
    "#         \"n_frames\": float_feature(n_frames),\n",
    "#         \"label\": int64_feature(label),\n",
    "#     })\n",
    "\n",
    "#     example = tf.train.SequenceExample(context=context_features, feature_lists=sequence_features)\n",
    "    \n",
    "#     return example\n",
    "\n",
    "# sequence_features = {\n",
    "#   \"frames\": tf.io.FixedLenSequenceFeature([], dtype=tf.float32)\n",
    "# }\n",
    "\n",
    "# context_features = {\n",
    "#   \"n_frames\": tf.io.FixedLenFeature([], tf.float32),\n",
    "#   \"label\": tf.io.FixedLenFeature([], tf.int64),\n",
    "# }\n",
    "\n",
    "\n",
    "# context, sequence = tf.io.parse_single_sequence_example(\n",
    "#     example,\n",
    "#     context_features=context_features, \n",
    "#     sequence_features=sequence_features\n",
    "# )\n",
    "\n",
    "\n",
    "# def parse_tfrecord_fn(example):\n",
    "#     feature_description = {\n",
    "#         \"n_frames\": tf.io.FixedLenFeature([], tf.float32),\n",
    "#         \"frames\": tf.io.FixedLenFeature([], tf.string),\n",
    "#         \"label\": tf.io.FixedLenFeature([], tf.int64),\n",
    "#     }\n",
    "#     example = tf.io.parse_single_example(example, feature_description)\n",
    "    \n",
    "#     n_frames = example[\"n_frames\"]\n",
    "#     label = tf.one_hot(example[\"label\"], depth=250)\n",
    "#     frames = tf.reshape(parse_sequence(example[\"frames\"]), shape=(n_frames, 543, 3))\n",
    "    \n",
    "#     return example\n",
    "\n",
    "def parse_tfrecord_fn(example):\n",
    "    feature_description = {\n",
    "        \"n_frames\": tf.io.FixedLenFeature([], tf.float32),\n",
    "        \"frames\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"label\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    \n",
    "    return tf.io.parse_single_example(example, feature_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fd25f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = create_example(\n",
    "    n_frames,\n",
    "    frames,\n",
    "    label\n",
    ")\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b54e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.io.TFRecordWriter(\n",
    "    tfrecords_dir + \"sample.tfrec\"\n",
    ") as writer:\n",
    "    example = create_example(\n",
    "        n_frames,\n",
    "        frames,\n",
    "        label\n",
    "    )\n",
    "    writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "509651ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/tfrecord_heatmaps_tmp/2044_chunk_0.tfrec'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfrec_path = \"../data/tfrecord_heatmaps_tmp/2044_chunk_0.tfrec\"\n",
    "tfrec_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11866186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TFRecordDatasetV2 element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset = tf.data.TFRecordDataset(tfrec_path)\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "317d83ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = parse_tfrecord_fn(next(iter(raw_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d6f5624f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sequence(serialized_sequence):\n",
    "    return tf.io.parse_tensor(\n",
    "        serialized_sequence,\n",
    "        out_type=tf.float16,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7ffa0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = tf.reshape(parse_sequence(example[\"frames\"]), shape=(28, 81, 48, 48)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "028401e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float16)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "19ff2591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_heatmaps(heatmaps, channel=-1, ratio=8):\n",
    "    # if channel is -1, draw all keypoints / limbs on the same map\n",
    "    import matplotlib.cm as cm\n",
    "    heatmaps = [x.transpose(1, 2, 0) for x in heatmaps]\n",
    "    h, w, _ = heatmaps[0].shape\n",
    "    newh, neww = int(h * ratio), int(w * ratio)\n",
    "\n",
    "    if channel == -1:\n",
    "        heatmaps = [np.max(x, axis=-1) for x in heatmaps]\n",
    "    cmap = cm.viridis\n",
    "    heatmaps = [(cmap(x)[..., :3] * 255).astype(np.uint8) for x in heatmaps]\n",
    "    heatmaps = [cv2.resize(x, (neww, newh)) for x in heatmaps]\n",
    "    return heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a2fcbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MovieWriter imagemagick unavailable; using Pillow instead.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 8))\n",
    "\n",
    "arr = frames\n",
    "\n",
    "heatmap = vis_heatmaps(arr)\n",
    "\n",
    "def update(i):\n",
    "    im_normed = heatmap[i]\n",
    "    ax.imshow(im_normed)\n",
    "    ax.set_title(f\"human_{i}\", fontsize=20)\n",
    "    ax.set_axis_off()\n",
    "\n",
    "\n",
    "anim = FuncAnimation(fig, update, frames=np.arange(0, len(arr)), interval=10)\n",
    "anim.save('humanpose3d-resized2.gif', dpi=80, writer='imagemagick')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ef93cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22372fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcab234c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=28.0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[\"n_frames\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d87ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.one_hot(example[\"label\"], depth=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba5a0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reshape(parse_sequence(example[\"frames\"]), shape=(example[\"n_frames\"], 543, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f64164",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(parse_sequence(example[\"frames\"]).numpy()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d4b84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trp = tfrecords_dir+f\"/stratified_split_{1}.tfrec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925e921c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = tf.data.TFRecordDataset(tfrec_path)\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109e0d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = parse_tfrecord_fn(next(iter(raw_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6705d793",
   "metadata": {},
   "outputs": [],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9e6a4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
