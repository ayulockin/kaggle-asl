{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c2ab37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-16 05:53:28.436065: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-16 05:53:28.635139: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-04-16 05:53:29.565935: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-04-16 05:53:29.566100: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-04-16 05:53:29.566117: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "import random\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from argparse import Namespace\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41c8b606",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_JOINTS = 543\n",
    "IMG_H = 56\n",
    "IMG_W = 56\n",
    "\n",
    "LIP = [\n",
    "    61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n",
    "    291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n",
    "    78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
    "    95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n",
    "]\n",
    "\n",
    "RIGHT_EYE = [\n",
    "    246, 161, 160, 159, 158, 157, 173,\n",
    "    33, 7, 163, 144, 145, 153, 154, 155, 133,\n",
    "    247, 30, 29, 27, 28, 56, 190,\n",
    "    130, 25, 110, 24, 23, 22, 26, 112, 243,\n",
    "    113, 225, 224, 223, 222, 221, 189,\n",
    "    226, 31, 228, 229, 230, 231, 232, 233, 244,\n",
    "    143, 111, 117, 118, 119, 120, 121, 128, 245,\n",
    "]\n",
    "\n",
    "LEFT_EYE = [\n",
    "    466, 387, 386, 385, 384, 398,\n",
    "    263, 249, 390, 373, 374, 380, 381, 382, 362,\n",
    "    467, 260, 259, 257, 258, 286, 414,\n",
    "    359, 255, 339, 254, 253, 252, 256, 341, 463,\n",
    "    342, 445, 444, 443, 442, 441, 413,\n",
    "    446, 261, 448, 449, 450, 451, 452, 453, 464,\n",
    "    372, 340, 346, 347, 348, 349, 350, 357, 465,\n",
    "]\n",
    "\n",
    "LEFT_HAND = [\n",
    "    468, 469, 470, 471, 472, 473, 474, 475,\n",
    "    476, 477, 478, 479, 480, 481, 482, 483,\n",
    "    484, 485, 486, 487, 488\n",
    "]\n",
    "\n",
    "RIGHT_HAND = [\n",
    "    522, 523, 524, 525, 526, 527, 528, 529,\n",
    "    530, 531, 532, 533, 534, 535, 536, 537,\n",
    "    538, 539, 540, 541, 542\n",
    "]\n",
    "\n",
    "POSE = [\n",
    "    489, 490, 491, 492, 493, 494, 495, 496, 497,\n",
    "    498, 499, 500, 501, 502, 503, 504, 505, 506,\n",
    "    507, 508, 509, 510, 511, 512, 513, 514, 515,\n",
    "    516, 517, 518, 519, 520, 521\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b48a63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data/tfrecords/stratified_split_0.tfrec', '../data/tfrecords/stratified_split_1.tfrec', '../data/tfrecords/stratified_split_2.tfrec', '../data/tfrecords/stratified_split_3.tfrec', '../data/tfrecords/stratified_split_4.tfrec', '../data/tfrecords/stratified_split_5.tfrec', '../data/tfrecords/stratified_split_6.tfrec', '../data/tfrecords/stratified_split_7.tfrec', '../data/tfrecords/stratified_split_8.tfrec', '../data/tfrecords/stratified_split_9.tfrec', '../data/tfrecords/stratified_split_10.tfrec', '../data/tfrecords/stratified_split_11.tfrec', '../data/tfrecords/stratified_split_12.tfrec', '../data/tfrecords/stratified_split_13.tfrec', '../data/tfrecords/stratified_split_14.tfrec', '../data/tfrecords/stratified_split_15.tfrec', '../data/tfrecords/stratified_split_16.tfrec', '../data/tfrecords/stratified_split_17.tfrec', '../data/tfrecords/stratified_split_18.tfrec', '../data/tfrecords/stratified_split_19.tfrec', '../data/tfrecords/stratified_split_20.tfrec', '../data/tfrecords/stratified_split_21.tfrec', '../data/tfrecords/stratified_split_22.tfrec', '../data/tfrecords/stratified_split_23.tfrec']\n"
     ]
    }
   ],
   "source": [
    "def natural_keys(text):\n",
    "    \"\"\n",
    "    def atoi(text):\n",
    "        return int(text) if text.isdigit() else text\n",
    "    \n",
    "    return [atoi(c) for c in re.split(r'(\\d+)', text)]\n",
    "\n",
    "tfrecords = glob(\"../data/tfrecords/*.tfrec\")\n",
    "tfrecords = sorted(tfrecords, key=natural_keys)\n",
    "print(tfrecords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6e689d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sequence(serialized_sequence):\n",
    "    return tf.io.parse_tensor(\n",
    "        serialized_sequence,\n",
    "        out_type=tf.float32,\n",
    "    )\n",
    "\n",
    "\n",
    "def parse_tfrecord_fn(example):\n",
    "    feature_description = {\n",
    "        \"n_frames\": tf.io.FixedLenFeature([], tf.float32),\n",
    "        \"frames\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"label\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    \n",
    "    return tf.io.parse_single_example(example, feature_description)\n",
    "\n",
    "\n",
    "def parse_data(example):\n",
    "    # Parse Frames\n",
    "    n_frames = example[\"n_frames\"]\n",
    "    frames = tf.reshape(parse_sequence(example[\"frames\"]), shape=(n_frames, 543, 3))\n",
    "\n",
    "    # Parse Labels\n",
    "    label = example[\"label\"]\n",
    "\n",
    "    return n_frames, frames, label\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "\n",
    "ds = tf.data.TFRecordDataset(tfrecords)\n",
    "\n",
    "dataloader = (\n",
    "    ds\n",
    "    .map(parse_tfrecord_fn, num_parallel_calls=AUTOTUNE)\n",
    "    .map(parse_data, num_parallel_calls=AUTOTUNE)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f7d086d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def generate_a_heatmap(arr, centers, max_values):\n",
    "    sigma = 0.1\n",
    "    img_h, img_w = arr.shape\n",
    "\n",
    "    for center, max_value in zip(centers, max_values):\n",
    "        mu_x, mu_y = center[0], center[1]\n",
    "        if not (np.isnan(mu_x) and np.isnan(mu_y)):\n",
    "            # scale\n",
    "            mu_x = min(math.floor(mu_x * img_w), img_w - 1)\n",
    "            mu_y = min(math.floor(mu_y * img_h), img_h - 1)\n",
    "\n",
    "            st_x = max(int(mu_x - 0.5 * sigma), 0)\n",
    "            ed_x = min(int(mu_x + 0.5 * sigma) + 1, img_w)\n",
    "            st_y = max(int(mu_y - 0.5 * sigma), 0)\n",
    "            ed_y = min(int(mu_y + 0.5 * sigma) + 1, img_h)\n",
    "            x = np.arange(st_x, ed_x, 1, np.float32)\n",
    "            y = np.arange(st_y, ed_y, 1, np.float32)\n",
    "\n",
    "            # if the keypoint not in the heatmap coordinate system\n",
    "            if not (len(x) and len(y)):\n",
    "                continue\n",
    "            y = y[:, None]\n",
    "\n",
    "            patch = np.exp(-((x - mu_x)**2 + (y - mu_y)**2) / 2 / sigma**2)\n",
    "            patch = patch * max_value\n",
    "            arr[st_y:ed_y, st_x:ed_x] = np.maximum(arr[st_y:ed_y, st_x:ed_x], patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42b29786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "# def get_3d_heatmap(ret, human_kps, num_frames):\n",
    "    \n",
    "#     for i, frame in enumerate(range(num_frames)):\n",
    "#         arr = ret[i]\n",
    "#         human = human_kps[i]\n",
    "\n",
    "#         x, y = human[:,:1], human[:,1:2]\n",
    "\n",
    "#         # TODO: Normalize the whole sequence together\n",
    "#         x = (x-np.nanmin(x))/(np.nanmax(x)-np.nanmin(x))\n",
    "#         y = (y-np.nanmin(y))/(np.nanmax(y)-np.nanmin(y))\n",
    "\n",
    "#         human = np.squeeze(np.array(list(zip(x, y))), axis=-1)\n",
    "\n",
    "#         kps = np.expand_dims(human, axis=0)\n",
    "#         all_kpscores = np.ones((1,num_frames,NUM_JOINTS), dtype=np.float32)\n",
    "#         kpscores = np.ones_like(all_kpscores[:, 0])\n",
    "\n",
    "#         for i in range(107):\n",
    "#             generate_a_heatmap(arr[i], kps[:, i], kpscores[:, i])\n",
    "            \n",
    "#     return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6bd38ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "61fede3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_3d_heatmap(ret, humans, num_frames, sigma=0.2):\n",
    "    x, y = humans[:,:,:1], humans[:,:,1:2]\n",
    "    print(x, y)\n",
    "\n",
    "    # MinMax Normalization\n",
    "    x_min = tf.reduce_min(tf.where(tf.math.is_nan(x), tf.float32.max, x))\n",
    "    x_max = tf.reduce_max(tf.where(tf.math.is_nan(x), tf.float32.min, x))\n",
    "    y_min = tf.reduce_min(tf.where(tf.math.is_nan(y), tf.float32.max, y))\n",
    "    y_max = tf.reduce_max(tf.where(tf.math.is_nan(y), tf.float32.min, y))\n",
    "\n",
    "    x = (x - x_min) / (x_max - x_min)\n",
    "    y = (y - y_min) / (y_max - y_min)\n",
    "\n",
    "    # Scale\n",
    "    x = tf.math.floor(x * IMG_W)\n",
    "    y = tf.math.floor(y * IMG_H)\n",
    "\n",
    "    # Get NaN mask\n",
    "    kps = tf.concat([x, y], axis=-1)\n",
    "    nan_mask = tf.math.is_nan(kps)\n",
    "\n",
    "    st_x = tf.math.maximum(tf.cast(x - 0.5 * sigma, tf.int32), tf.constant(0, dtype=tf.int32))\n",
    "    ed_x = tf.math.minimum(tf.cast(x + 0.5 * sigma, tf.int32) + 1, tf.cast(IMG_W, tf.int32))\n",
    "    st_y = tf.math.maximum(tf.cast(y - 0.5 * sigma, tf.int32), tf.constant(0, dtype=tf.int32))\n",
    "    ed_y = tf.math.minimum(tf.cast(y + 0.5 * sigma, tf.int32) + 1, tf.cast(IMG_H, tf.int32))\n",
    "\n",
    "    i = tf.concat([st_x, ed_x-1], axis=-1)\n",
    "    j = tf.concat([st_y, ed_y-1], axis=-1)[..., None]\n",
    "\n",
    "    ix = (i - tf.cast(x, tf.int32))**2\n",
    "    ix = tf.broadcast_to(tf.transpose(ix[..., tf.newaxis], (0,1,3,2)), (32, 107, 2, 2))\n",
    "    jy = (j - tf.cast(y,tf.int32)[..., tf.newaxis])**2\n",
    "\n",
    "    patch = tf.exp(-(ix+jy) / 2 / sigma**2)\n",
    "    \n",
    "    return patch, i, j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "174a73b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose2heatmap(example):\n",
    "    num_frames, data, label = example\n",
    "    \n",
    "    faces = tf.gather(data, LIP, axis=1).numpy()\n",
    "    poses = tf.gather(data, POSE, axis=1).numpy()[:,:-8]\n",
    "    rhs = tf.gather(data, RIGHT_HAND, axis=1).numpy()\n",
    "    lhs = tf.gather(data, LEFT_HAND, axis=1).numpy()\n",
    "\n",
    "    humans = tf.concat([faces, poses, rhs, lhs], axis=1)\n",
    "    \n",
    "    # Sample frames - resize or pad/uniform sample. Doing simple resize.\n",
    "    humans = tf.image.resize(humans, size=(32, 107), method=\"nearest\")\n",
    "    \n",
    "    # Empty tensor for heatmap\n",
    "    ret = tf.zeros((32, 107, IMG_H, IMG_W), dtype=tf.float32)\n",
    "\n",
    "#     heatmap = get_3d_heatmap(ret, humans, num_frames)\n",
    "    \n",
    "#     return heatmap, label\n",
    "    return ret, humans, num_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "bc9b1523",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(23, 543, 3), dtype=float32, numpy=\n",
       "array([[[ 0.49440014,  0.38046983, -0.03062646],\n",
       "        [ 0.49601725,  0.3507348 , -0.05756483],\n",
       "        [ 0.5008185 ,  0.35934305, -0.03028346],\n",
       "        ...,\n",
       "        [ 0.31373617,  0.41234398, -0.05269891],\n",
       "        [ 0.35072815,  0.39958185, -0.06021732],\n",
       "        [ 0.38579622,  0.4011007 , -0.06471767]],\n",
       "\n",
       "       [[ 0.5011503 ,  0.38055426, -0.03156953],\n",
       "        [ 0.49290648,  0.3493601 , -0.05817606],\n",
       "        [ 0.49832708,  0.3581275 , -0.03118932],\n",
       "        ...,\n",
       "        [ 0.33588555,  0.3889878 , -0.0631762 ],\n",
       "        [ 0.37742943,  0.3802519 , -0.07176355],\n",
       "        [ 0.41639116,  0.38289747, -0.076529  ]],\n",
       "\n",
       "       [[ 0.49847096,  0.37949273, -0.0309729 ],\n",
       "        [ 0.49176967,  0.34847346, -0.05721497],\n",
       "        [ 0.49767977,  0.3573629 , -0.03084622],\n",
       "        ...,\n",
       "        [ 0.35855788,  0.3844513 , -0.06097187],\n",
       "        [ 0.4005313 ,  0.37769613, -0.07045607],\n",
       "        [ 0.43907768,  0.37987   , -0.07860945]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.5357876 ,  0.37571952, -0.04070858],\n",
       "        [ 0.5371784 ,  0.34479237, -0.06370237],\n",
       "        [ 0.5389905 ,  0.3552047 , -0.03697387],\n",
       "        ...,\n",
       "        [        nan,         nan,         nan],\n",
       "        [        nan,         nan,         nan],\n",
       "        [        nan,         nan,         nan]],\n",
       "\n",
       "       [[ 0.53569597,  0.37659568, -0.04069101],\n",
       "        [ 0.53505826,  0.34505755, -0.06355848],\n",
       "        [ 0.53723085,  0.35539716, -0.0369088 ],\n",
       "        ...,\n",
       "        [        nan,         nan,         nan],\n",
       "        [        nan,         nan,         nan],\n",
       "        [        nan,         nan,         nan]],\n",
       "\n",
       "       [[ 0.5364163 ,  0.37519765, -0.0414404 ],\n",
       "        [ 0.53597146,  0.34245962, -0.06361609],\n",
       "        [ 0.5378655 ,  0.35314763, -0.03705885],\n",
       "        ...,\n",
       "        [ 0.00672346,  0.6650444 , -0.11401682],\n",
       "        [-0.01475476,  0.6437988 , -0.12348831],\n",
       "        [-0.03181136,  0.62707704, -0.12906739]]], dtype=float32)>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(iter(dataloader))\n",
    "sample[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c0619fe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([32, 107, 56, 56]), TensorShape([32, 107, 3]))"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret, humans, _ = pose2heatmap(sample)\n",
    "ret.shape, humans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "8a1c1639",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 107, 3), dtype=float32, numpy=\n",
       "array([[[ 4.6895200e-01,  3.9122039e-01,  1.3318895e-02],\n",
       "        [ 4.6855748e-01,  3.8820282e-01,  3.4378599e-03],\n",
       "        [ 4.7007626e-01,  3.8500535e-01, -6.9671771e-03],\n",
       "        ...,\n",
       "        [           nan,            nan,            nan],\n",
       "        [           nan,            nan,            nan],\n",
       "        [           nan,            nan,            nan]],\n",
       "\n",
       "       [[ 4.6608999e-01,  3.9358661e-01,  1.3970846e-02],\n",
       "        [ 4.6797937e-01,  3.9064983e-01,  3.9357482e-03],\n",
       "        [ 4.7196886e-01,  3.8727656e-01, -6.8762675e-03],\n",
       "        ...,\n",
       "        [           nan,            nan,            nan],\n",
       "        [           nan,            nan,            nan],\n",
       "        [           nan,            nan,            nan]],\n",
       "\n",
       "       [[ 4.6608999e-01,  3.9358661e-01,  1.3970846e-02],\n",
       "        [ 4.6797937e-01,  3.9064983e-01,  3.9357482e-03],\n",
       "        [ 4.7196886e-01,  3.8727656e-01, -6.8762675e-03],\n",
       "        ...,\n",
       "        [           nan,            nan,            nan],\n",
       "        [           nan,            nan,            nan],\n",
       "        [           nan,            nan,            nan]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 5.0507236e-01,  3.9752603e-01,  1.8367120e-03],\n",
       "        [ 5.0595009e-01,  3.9218751e-01, -8.3285049e-03],\n",
       "        [ 5.0875920e-01,  3.8651186e-01, -1.8690461e-02],\n",
       "        ...,\n",
       "        [           nan,            nan,            nan],\n",
       "        [           nan,            nan,            nan],\n",
       "        [           nan,            nan,            nan]],\n",
       "\n",
       "       [[ 5.0507236e-01,  3.9752603e-01,  1.8367120e-03],\n",
       "        [ 5.0595009e-01,  3.9218751e-01, -8.3285049e-03],\n",
       "        [ 5.0875920e-01,  3.8651186e-01, -1.8690461e-02],\n",
       "        ...,\n",
       "        [           nan,            nan,            nan],\n",
       "        [           nan,            nan,            nan],\n",
       "        [           nan,            nan,            nan]],\n",
       "\n",
       "       [[ 5.0471312e-01,  3.9823407e-01,  2.2524806e-04],\n",
       "        [ 5.0570840e-01,  3.9240596e-01, -9.8894360e-03],\n",
       "        [ 5.0877547e-01,  3.8615888e-01, -2.0065878e-02],\n",
       "        ...,\n",
       "        [           nan,            nan,            nan],\n",
       "        [           nan,            nan,            nan],\n",
       "        [           nan,            nan,            nan]]], dtype=float32)>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "humans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d6d08431",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# patch, i, j = get_3d_heatmap(rett, humanst, num_framest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "19c906ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([32, 107, 1]), TensorShape([32, 107, 1]))"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = humans[:,:,:1], humans[:,:,1:2]\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "01123f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMax Normalization\n",
    "x_min = tf.reduce_min(tf.where(tf.math.is_nan(x), tf.float32.max, x))\n",
    "x_max = tf.reduce_max(tf.where(tf.math.is_nan(x), tf.float32.min, x))\n",
    "y_min = tf.reduce_min(tf.where(tf.math.is_nan(y), tf.float32.max, y))\n",
    "y_max = tf.reduce_max(tf.where(tf.math.is_nan(y), tf.float32.min, y))\n",
    "\n",
    "x = (x - x_min) / (x_max - x_min)\n",
    "y = (y - y_min) / (y_max - y_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e4667601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([32, 107, 1]), TensorShape([32, 107, 1]))"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "02fb2b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale\n",
    "x = tf.math.floor(x * IMG_W)\n",
    "y = tf.math.floor(y * IMG_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "388e1e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([32, 107, 1]), TensorShape([32, 107, 1]))"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "bcc44072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 107, 2])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kps = tf.concat([x, y], axis=-1)\n",
    "kps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "683775bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 0.2\n",
    "st_x = tf.math.maximum(tf.cast(x - 0.5 * sigma, tf.int32), tf.constant(0, dtype=tf.int32))\n",
    "ed_x = tf.math.minimum(tf.cast(x + 0.5 * sigma, tf.int32) + 1, tf.cast(IMG_W, tf.int32))\n",
    "st_y = tf.math.maximum(tf.cast(y - 0.5 * sigma, tf.int32), tf.constant(0, dtype=tf.int32))\n",
    "ed_y = tf.math.minimum(tf.cast(y + 0.5 * sigma, tf.int32) + 1, tf.cast(IMG_H, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "3ad8bec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1,), dtype=int32, numpy=array([20], dtype=int32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([22], dtype=int32)>)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st_x[0][0], ed_x[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "88bcadf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1,), dtype=int32, numpy=array([5], dtype=int32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([7], dtype=int32)>)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st_y[0][0], ed_y[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37ded9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = tf.concat([st_x, ed_x-1], axis=-1)\n",
    "j = tf.concat([st_y, ed_y-1], axis=-1)[..., None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bf6117",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717d5d58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141a75b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "ix = (i - tf.cast(x, tf.int32))**2\n",
    "ix = tf.broadcast_to(tf.transpose(ix[..., tf.newaxis], (0,1,3,2)), (32, 107, 2, 2))\n",
    "jy = (j - tf.cast(y,tf.int32)[..., tf.newaxis])**2\n",
    "\n",
    "patch = tf.exp(-(ix+jy) / 2 / sigma**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935dd3d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc88df3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b2f74e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c0fc0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17511ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce63cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e79e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be7badc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099befda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1a078b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb84c53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324affd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05e7e81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d330eab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05175c63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d66aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3470540a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb67d1e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383874f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29eb4bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307bd608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3034c09e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
