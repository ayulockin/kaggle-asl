{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5907047",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import re\n",
    "import cv2\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import multiprocessing \n",
    "from argparse import Namespace\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "390995ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.csv file\n",
    "df = pd.read_csv(\"../data/train.csv\")\n",
    "\n",
    "# Get labels 2 id\n",
    "with open(\"../data/sign_to_prediction_index_map.json\") as f:\n",
    "    label2id = json.load(f)\n",
    "\n",
    "\n",
    "def add_path(row):\n",
    "    return \"../data/\"+row.path\n",
    "    \n",
    "df[\"sign\"] = df[\"sign\"].apply(lambda sign: label2id[sign])\n",
    "df[\"path\"] = df.apply(lambda row: add_path(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d8fff4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "NUM_JOINTS = 81\n",
    "IMG_H = 48\n",
    "IMG_W = 48\n",
    "tfrecords_dir = \"../data/tfrecord_heatmaps_tmp\"\n",
    "os.makedirs(tfrecords_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "LIP = [\n",
    "    61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n",
    "    291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n",
    "]\n",
    "\n",
    "RIGHT_EYE = [\n",
    "    246, 161, 160, 159, 158, 157, 173,\n",
    "    33, 7, 163, 144, 145, 153, 154, 155, 133,\n",
    "    247, 30, 29, 27, 28, 56, 190,\n",
    "    130, 25, 110, 24, 23, 22, 26, 112, 243,\n",
    "    113, 225, 224, 223, 222, 221, 189,\n",
    "    226, 31, 228, 229, 230, 231, 232, 233, 244,\n",
    "    143, 111, 117, 118, 119, 120, 121, 128, 245,\n",
    "]\n",
    "\n",
    "LEFT_EYE = [\n",
    "    466, 387, 386, 385, 384, 398,\n",
    "    263, 249, 390, 373, 374, 380, 381, 382, 362,\n",
    "    467, 260, 259, 257, 258, 286, 414,\n",
    "    359, 255, 339, 254, 253, 252, 256, 341, 463,\n",
    "    342, 445, 444, 443, 442, 441, 413,\n",
    "    446, 261, 448, 449, 450, 451, 452, 453, 464,\n",
    "    372, 340, 346, 347, 348, 349, 350, 357, 465,\n",
    "]\n",
    "\n",
    "LEFT_HAND = [\n",
    "    468, 469, 470, 471, 472, 473, 474, 475,\n",
    "    476, 477, 478, 479, 480, 481, 482, 483,\n",
    "    484, 485, 486, 487, 488\n",
    "]\n",
    "\n",
    "RIGHT_HAND = [\n",
    "    522, 523, 524, 525, 526, 527, 528, 529,\n",
    "    530, 531, 532, 533, 534, 535, 536, 537,\n",
    "    538, 539, 540, 541, 542\n",
    "]\n",
    "\n",
    "POSE = [\n",
    "    489, 490, 491, 492, 493, 494, 495, 496, 497,\n",
    "    498, 499, 500, 501, 502, 503, 504, 505, 512,\n",
    "    513, 514, 515, 516, 517, 518, 519, 520, 521\n",
    "]\n",
    "\n",
    "\n",
    "def generate_a_heatmap(arr, centers, max_values):\n",
    "    \"\"\"Generate pseudo heatmap for one keypoint in one frame.\n",
    "\n",
    "    Args:\n",
    "        arr (np.ndarray): The array to store the generated heatmaps. Shape: img_h * img_w.\n",
    "        centers (np.ndarray): The coordinates of corresponding keypoints (of multiple persons). Shape: M * 2.\n",
    "        max_values (np.ndarray): The max values of each keypoint. Shape: M.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The generated pseudo heatmap.\n",
    "    \"\"\"\n",
    "\n",
    "    sigma = 0.9\n",
    "    img_h, img_w = arr.shape\n",
    "\n",
    "    for center, max_value in zip(centers, max_values):\n",
    "        mu_x, mu_y = center[0], center[1]\n",
    "        if not (np.isnan(mu_x) and np.isnan(mu_y)):\n",
    "            # scale\n",
    "            mu_x = min(math.floor(mu_x * img_w), img_w - 1)\n",
    "            mu_y = min(math.floor(mu_y * img_h), img_h - 1)\n",
    "\n",
    "            st_x = max(int(mu_x - 2 * sigma), 0)\n",
    "            ed_x = min(int(mu_x + 2 * sigma) + 1, img_w)\n",
    "            st_y = max(int(mu_y - 2 * sigma), 0)\n",
    "            ed_y = min(int(mu_y + 2 * sigma) + 1, img_h)\n",
    "            x = np.arange(st_x, ed_x, 1, np.float32)\n",
    "            y = np.arange(st_y, ed_y, 1, np.float32)\n",
    "\n",
    "            # if the keypoint not in the heatmap coordinate system\n",
    "            if not (len(x) and len(y)):\n",
    "                continue\n",
    "            y = y[:, None]\n",
    "\n",
    "            patch = np.exp(-((x - mu_x)**2 + (y - mu_y)**2) / 2 / sigma**2)\n",
    "            patch = patch * max_value\n",
    "            arr[st_y:ed_y, st_x:ed_x] = np.maximum(arr[st_y:ed_y, st_x:ed_x], patch)\n",
    "\n",
    "\n",
    "def get_3d_heatmap(ret, human_kps, num_frames):\n",
    "    \n",
    "    for i, frame in enumerate(range(num_frames)):\n",
    "        arr = ret[i]\n",
    "        human = human_kps[i]\n",
    "\n",
    "        x, y = human[:,:1], human[:,1:2]\n",
    "\n",
    "        # TODO: Normalize the whole sequence together\n",
    "        x = (x-np.nanmin(x))/(np.nanmax(x)-np.nanmin(x))\n",
    "        y = (y-np.nanmin(y))/(np.nanmax(y)-np.nanmin(y))\n",
    "\n",
    "        human = np.squeeze(np.array(list(zip(x, y))), axis=-1)\n",
    "\n",
    "        kps = np.expand_dims(human, axis=0)\n",
    "        all_kpscores = np.ones((1,num_frames,NUM_JOINTS), dtype=np.float32)\n",
    "        kpscores = np.ones_like(all_kpscores[:, 0])\n",
    "\n",
    "        num_kp = kps.shape[1]\n",
    "        for i in range(num_kp):\n",
    "            generate_a_heatmap(arr[i], kps[:, i], kpscores[:, i])\n",
    "            \n",
    "    return ret\n",
    "\n",
    "\n",
    "def float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "\n",
    "def int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "def bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value.numpy()]))\n",
    "\n",
    "\n",
    "def serialize_sequence(sequence):\n",
    "    \"\"\"Serialize the multidimentional tensor\"\"\"\n",
    "    return tf.io.serialize_tensor(sequence)\n",
    "\n",
    "\n",
    "def parse_sequence(serialized_sequence):\n",
    "    return tf.io.parse_tensor(\n",
    "        serialized_sequence,\n",
    "        out_type=tf.float16,\n",
    "    )\n",
    "\n",
    "\n",
    "def create_example(n_frames, sequence, label):\n",
    "    feature = {\n",
    "        \"n_frames\": float_feature(n_frames),\n",
    "        \"frames\": bytes_feature(serialize_sequence(sequence)),\n",
    "        \"label\": int64_feature(label),\n",
    "    }\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4417977c",
   "metadata": {},
   "outputs": [],
   "source": [
    "participants_group = df.groupby(\"participant_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a925ad46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__orig_bases__',\n",
       " '__parameters__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_accessors',\n",
       " '_agg_examples_doc',\n",
       " '_agg_general',\n",
       " '_agg_py_fallback',\n",
       " '_aggregate_frame',\n",
       " '_aggregate_item_by_item',\n",
       " '_aggregate_with_numba',\n",
       " '_apply_allowlist',\n",
       " '_apply_filter',\n",
       " '_apply_to_column_groupbys',\n",
       " '_bool_agg',\n",
       " '_can_use_transform_fast',\n",
       " '_choose_path',\n",
       " '_concat_objects',\n",
       " '_constructor',\n",
       " '_cumcount_array',\n",
       " '_cython_agg_general',\n",
       " '_cython_transform',\n",
       " '_define_paths',\n",
       " '_dir_additions',\n",
       " '_dir_deletions',\n",
       " '_fill',\n",
       " '_get_cythonized_result',\n",
       " '_get_data_to_aggregate',\n",
       " '_get_index',\n",
       " '_get_indices',\n",
       " '_gotitem',\n",
       " '_group_selection',\n",
       " '_hidden_attrs',\n",
       " '_insert_inaxis_grouper_inplace',\n",
       " '_internal_names',\n",
       " '_internal_names_set',\n",
       " '_iterate_column_groupbys',\n",
       " '_iterate_slices',\n",
       " '_make_wrapper',\n",
       " '_numba_prep',\n",
       " '_obj_1d_constructor',\n",
       " '_obj_with_exclusions',\n",
       " '_python_agg_general',\n",
       " '_python_apply_general',\n",
       " '_reindex_output',\n",
       " '_reset_cache',\n",
       " '_reset_group_selection',\n",
       " '_resolve_numeric_only',\n",
       " '_selected_obj',\n",
       " '_selection',\n",
       " '_selection_list',\n",
       " '_set_group_selection',\n",
       " '_set_result_index_ordered',\n",
       " '_transform',\n",
       " '_transform_general',\n",
       " '_transform_item_by_item',\n",
       " '_transform_with_numba',\n",
       " '_wrap_agged_manager',\n",
       " '_wrap_aggregated_output',\n",
       " '_wrap_applied_output',\n",
       " '_wrap_applied_output_series',\n",
       " '_wrap_transform_fast_result',\n",
       " '_wrap_transformed_output',\n",
       " 'agg',\n",
       " 'aggregate',\n",
       " 'all',\n",
       " 'any',\n",
       " 'apply',\n",
       " 'backfill',\n",
       " 'bfill',\n",
       " 'boxplot',\n",
       " 'corr',\n",
       " 'corrwith',\n",
       " 'count',\n",
       " 'cov',\n",
       " 'cumcount',\n",
       " 'cummax',\n",
       " 'cummin',\n",
       " 'cumprod',\n",
       " 'cumsum',\n",
       " 'describe',\n",
       " 'diff',\n",
       " 'dtypes',\n",
       " 'ewm',\n",
       " 'expanding',\n",
       " 'ffill',\n",
       " 'fillna',\n",
       " 'filter',\n",
       " 'first',\n",
       " 'get_group',\n",
       " 'groups',\n",
       " 'head',\n",
       " 'hist',\n",
       " 'idxmax',\n",
       " 'idxmin',\n",
       " 'indices',\n",
       " 'last',\n",
       " 'mad',\n",
       " 'max',\n",
       " 'mean',\n",
       " 'median',\n",
       " 'min',\n",
       " 'ndim',\n",
       " 'ngroup',\n",
       " 'ngroups',\n",
       " 'nth',\n",
       " 'nunique',\n",
       " 'ohlc',\n",
       " 'pad',\n",
       " 'participant_id',\n",
       " 'path',\n",
       " 'pct_change',\n",
       " 'pipe',\n",
       " 'plot',\n",
       " 'prod',\n",
       " 'quantile',\n",
       " 'rank',\n",
       " 'resample',\n",
       " 'rolling',\n",
       " 'sample',\n",
       " 'sem',\n",
       " 'sequence_id',\n",
       " 'shift',\n",
       " 'sign',\n",
       " 'size',\n",
       " 'skew',\n",
       " 'std',\n",
       " 'sum',\n",
       " 'tail',\n",
       " 'take',\n",
       " 'transform',\n",
       " 'tshift',\n",
       " 'var']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(participants_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edc1debe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "keys = list(participants_group.groups.keys())\n",
    "print(len(keys))\n",
    "idx = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d786db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28it [06:21, 14.48s/it]"
     ]
    }
   ],
   "source": [
    "participant_id = keys[idx]\n",
    "print(participant_id)\n",
    "participant_df = participants_group.get_group(keys[idx])\n",
    "\n",
    "paths = participant_df.path.values\n",
    "labels = participant_df.sign.values\n",
    "\n",
    "chunk_length = int(max(1, np.ceil(len(paths) / 50)))\n",
    "paths_chunk = [paths[x:x+chunk_length] for x in range(0, len(paths), chunk_length)]\n",
    "labels_chunk = [labels[x:x+chunk_length] for x in range(0, len(labels), chunk_length)]\n",
    "\n",
    "for idx, (chunks, labels) in tqdm(enumerate(zip(paths_chunk, labels_chunk))):\n",
    "    with tf.io.TFRecordWriter(\n",
    "        tfrecords_dir + f\"/{participant_id}_chunk_{idx}.tfrec\"\n",
    "    ) as writer:\n",
    "        for chunk, label in zip(chunks, labels):\n",
    "            data = pd.read_parquet(chunk)[[\"x\", \"y\", \"z\"]].values.astype(np.float32)\n",
    "            n_frames = int(len(data)/543)\n",
    "            data = np.reshape(data, newshape=(n_frames, 543, 3))\n",
    "\n",
    "            faces = tf.gather(data, LIP, axis=1).numpy()\n",
    "            poses = tf.gather(data, POSE, axis=1).numpy()[:,:-8]\n",
    "            rhs = tf.gather(data, RIGHT_HAND, axis=1).numpy()\n",
    "            lhs = tf.gather(data, LEFT_HAND, axis=1).numpy()\n",
    "\n",
    "            humans = np.concatenate([faces, poses, rhs, lhs], axis=1)\n",
    "            num_frames = humans.shape[0]\n",
    "\n",
    "            if num_frames < 28:\n",
    "                humans = tf.image.resize(humans, (28, humans.shape[1]), method=\"nearest\").numpy()\n",
    "            else:\n",
    "                # uniform sampling\n",
    "                indices = sorted(np.random.choice(num_frames, 28, replace=False))\n",
    "                humans = humans[indices]\n",
    "\n",
    "            num_frames = humans.shape[0]\n",
    "            ret = np.zeros([num_frames, NUM_JOINTS, IMG_H, IMG_W], dtype=np.float16)\n",
    "\n",
    "            heatmap = get_3d_heatmap(ret, humans, num_frames)\n",
    "\n",
    "            example = create_example(\n",
    "                num_frames,\n",
    "                heatmap,\n",
    "                label\n",
    "            )\n",
    "\n",
    "            writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215eeb6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af12fb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7e2fe7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c788a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b6e13b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a68961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a40d959",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
